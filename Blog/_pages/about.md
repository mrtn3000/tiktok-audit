---
layout: about
title: about
permalink: /about/
profile:
  align:
  image:
  image_circular: false # crops the image to make it circular
  address:

news: false  # includes a list of news items
latest_posts: false  # includes a list of the newest posts
selected_papers: false # includes a list of papers marked as "selected={true}"
social: false  # includes social icons at the bottom of the page
nav: true
nav_order: 1
---
### about us

This blog is a result of a research project on TikTok's by ([Kathy](https://www.kathymessmer.de/), [Martin](https://martin.degeling.com), [Anna](https://www.stiftung-nv.de/persons/anna-semenova), [Alex](https://www.stiftung-nv.de/en/person/alexander-hohlfeld)) and [Greta](https://www.interface-eu.org/persons/greta-hess), at [interface](https://www.interface-eu.org) (previously "Stiftung Neue Verantwortung), a non-profit think tank on information technology and public policy. Learn more about the project[here](https://www.interface-eu.org/focus-area/tiktok-audit).

The project ended in September 2024, since then the blog is maintained by Martin now at [AI Forensics](https://aiforensics.org/).

### about the project

Recommender systems play an increasingly important role in the lives of many people. Whether we use a search engine or social media platform, algorithms, artificial intelligence and statistical models determine what, when and how content is presented to users. Despite the importance of these systems for our daily lives and how we inform ourselves and communicate with each other, their design is challenging to understand for users, politicians, researchers, and civil society. 

Whether certain content is systematically disadvantaged or favored, whether recommender systems amplify hate and disinformation, and how user behavior, algorithms, and platform design intertwine are highly relevant questions. It has been shown that platforms can contribute to serious harm to individuals, groups and societies. Studies have suggested that these negative impacts range from worsening an individual’s mental health to driving society-wide polarisation capable of putting democracies at risk. 

To better safeguard people from these harms, the European Union’s Digital Services Act (DSA) requires platforms, especially those with large numbers of users, to make their algorithmic systems more transparent and follow due diligence obligations. However, the DSA lacks concrete guidelines. To fill this gap, we propose a **[risk-scenario-based audit process (RSBA process).](https://www.interface-eu.org/publications/auditing-recommender-systems-overview-existing-audits-risk-assessments-and-studies)** 

### about this website

[Our project](https://www.interface-eu.org/focus-area/approaches-to-analyse-and-evaluate-ai-based-recommendation-systems-for-internet-intermediaries) is dedicated to studying the impact of social media platforms by closely looking into their recommender systems. Within the framework of the project (RSBA process), and given that it impacts billions of users around the world, we have decided to concentrate on TikTok. Our approach considers the evolving nature of platforms and emphasises the observability of their recommender systems’ components. We have found and learned many interesting things along the way. 

On this website -through our [blog](https://tiktok-audit.com/blog/)- we document our journey into exploring, testing and auditing TikTok’s recommendation algorithms. Instead of writing long policy papers, this blog is the main space through which we want to share our analyses as they emerge with interested stakeholders and the general public.

Make sure to check out our [news](https://tiktok-audit.com/news/) section to keep up with what we have been up to! You can also follow udates on [Mastodon](https://mastodon.social/@tiktok_audit) or via [RSS](https://tiktok-audit.com/feed.xml).